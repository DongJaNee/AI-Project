<img width="1312" height="199" alt="image" src="https://github.com/user-attachments/assets/c72047e8-1493-42f7-bf9c-55808a18b8ec" />## embedding 
- 자연어 -> 숫자로 바꾸는 것 (LLM에서도 적용)
- 임베딩을 하려면 사전에 공부를 한 임베딩 모델을 써야함.
- 유사한 언어를 도출해내려면 embedding 값이 가까워야함.

## Vector DB
- embedding 한 결과를 DB에 저장
- 벡터 데이털르 저장하고 검색하기 위한 데이터베이스 SW

## Vector DB Chunk
- 데이터를 일정한 크기로 나누는 단위
- 텍스트 데이터를 **글자 수**나 **바이트 수** 기준으로 나눈다

## Langchain
- lang : Models, Prompts, chains
- chain : Embeddings & VectorStores, Agents

 ---
 # LLM 모델 파인튜닝
 ### 1. 허깅페이스 준비 
 ```
1. 허깅페이스에 로그인 또는 계정 생성 후 로그인
2. write 권한을 가지 토큰 생성(반드시 메모장에 저장하십시오. 생성 시에만 값을 볼 수 있습니다.)
3. 허깅페이스에 새로운 데이터셋을 업로드할 데이터셋 repository를 생성하십시오. 생성 방법은 링크를 클릭하십시오.
```

### 2. 데이터 셋 준비 
- GPT, Claud 등 open AI 를 사용하여 데이터 셋 500개 생성 (알파카 포맷으로 생성)
- 생성한 데이터를 raw_data.csv파일로 저장
- raw_data.csv 파일을 아래의 파이썬 코드를 이용하여 train, test 2개의 데이터셋으로,  9:1의 비율로 split하여 만들어진 데이터셋 2개를 각각 csv 파일로 저장

```
import pandas as pd

df_raw = pd.read_csv('raw_data.csv')
from sklearn.model_selection import train_test_split

df_train, df_test = train_test_split(df_raw, test_size=0.1, random_state=42)

df_train.to_csv('train_ds.csv', index=False)
df_test.to_csv('test_ds.csv', index=False)
```

### 3. csv 파일 2개를  하깅페이스에 업로드

<img width="1312" height="199" alt="image" src="https://github.com/user-attachments/assets/5c706139-1f95-48f5-8bf5-bf7fe4bae402" />

--- 
# 라마3 모델 파인튜닝 
### 1. 현재 CUDA 장치의 major 버전과 minor 버전을 조회
```
import torch

major_version, minor_version = torch.cuda.get_device_capability(0)
print(f"{torch.cuda.get_device_name(0)} Compute Capability = {major_version}.{minor_version}")
```

### 2. Unsloth 설치
```
%%capture
import os, re
if "COLAB_" not in "".join(os.environ.keys()):
    !pip install unsloth
else:
    # Do this only in Colab notebooks! Otherwise use pip install unsloth
    import torch; v = re.match(r"[0-9\.]{3,}", str(torch.__version__)).group(0)
    xformers = "xformers==" + ("0.0.32.post2" if v == "2.8.0" else "0.0.29.post3")
    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo
    !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" "huggingface_hub>=0.34.0" hf_transfer
    !pip install --no-deps unsloth
!pip install transformers==4.55.4
!pip install --no-deps trl==0.22.2
```

### 3. FastLanguageModel.from_pretrained 함수를 통해 모델과 토크나이저를 로드
```
from unsloth import FastLanguageModel
import torch

max_seq_length = 4096  # 최대 시퀀스 길이를 설정합니다. 내부적으로 RoPE 스케일링을 자동으로 지원합니다!
# 자동 감지를 위해 None을 사용합니다. Tesla T4, V100은 Float16, Ampere+는 Bfloat16을 사용하세요.
dtype = None
# 메모리 사용량을 줄이기 위해 4bit 양자화를 사용합니다. False일 수도 있습니다.
load_in_4bit = True

# 4배 빠른 다운로드와 메모리 부족 문제를 방지하기 위해 지원하는 4bit 사전 양자화 모델입니다.
fourbit_models = [
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/mistral-7b-instruct-v0.2-bnb-4bit",
    "unsloth/llama-2-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",
    "unsloth/gemma-7b-it-bnb-4bit",  # Gemma 7b의 Instruct 버전
    "unsloth/gemma-2b-bnb-4bit",
    "unsloth/gemma-2b-it-bnb-4bit",  # Gemma 2b의 Instruct 버전
    "unsloth/llama-3-8b-bnb-4bit",  # Llama-3 8B
]  # 더 많은 모델은 https://huggingface.co/unsloth 에서 확인할 수 있습니다.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="beomi/Llama-3-Open-Ko-8B-Instruct-preview",  # 모델 이름을 설정합니다.
    max_seq_length=max_seq_length,  # 최대 시퀀스 길이를 설정합니다.
    dtype=dtype,  # 데이터 타입을 설정합니다.
    load_in_4bit=load_in_4bit,  # 4bit 양자화 로드 여부를 설정합니다.
    # token = "hf_...", # 게이트된 모델을 사용하는 경우 토큰을 사용하세요. 예: meta-llama/Llama-2-7b-hf
)
```

### 4. LoRA 파인튜닝 모델 구성
```
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # 0보다 큰 어떤 숫자도 선택 가능! 8, 16, 32, 64, 128이 권장됩니다.
    lora_alpha=32,  # LoRA 알파 값을 설정합니다.
    lora_dropout=0.05,  # 드롭아웃을 지원합니다.
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],  # 타겟 모듈을 지정합니다.
    bias="none",  # 바이어스를 지원합니다.
    # True 또는 "unsloth"를 사용하여 매우 긴 컨텍스트에 대해 VRAM을 30% 덜 사용하고, 2배 더 큰 배치 크기를 지원합니다.
    use_gradient_checkpointing="unsloth",
    random_state=123,  # 난수 상태를 설정합니다.
    use_rslora=False,  # 순위 안정화 LoRA를 지원합니다.
    loftq_config=None,  # LoftQ를 지원합니다.
```

### 5. 데이터 사용
```
from datasets import load_dataset

dataset = load_dataset("huggingface.repo 이름", data_files="csv 파일 이름")
```

```
print(dataset.keys())
```

### 6. 데이터 셋 split
```
# Split the dataset into training/validation and test sets
train_val_split = dataset['train'].train_test_split(test_size=0.2)

# Print the keys of the split dataset to verify
print(train_val_split.keys())

train_ds = train_val_split['train']
eval_ds = train_val_split['test']
```

```
print(train_ds[0], '\n', eval_ds[0])
```

```
# EOS_TOKEN은 문장의 끝을 나타내는 토큰입니다. 이 토큰을 추가해야 합니다.
EOS_TOKEN = tokenizer.eos_token

# AlpacaPrompt를 사용하여 지시사항을 포맷팅하는 함수입니다.
alpaca_prompt = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{}

### Response:
{}"""


# 주어진 예시들을 포맷팅하는 함수입니다.
def formatting_prompts_func(examples):
    instructions = examples["instruction"]  # 지시사항을 가져옵니다.
    outputs = examples["output"]  # 출력값을 가져옵니다.
    texts = []  # 포맷팅된 텍스트를 저장할 리스트입니다.
    for instruction, output in zip(instructions, outputs):
        # EOS_TOKEN을 추가해야 합니다. 그렇지 않으면 생성이 무한히 진행될 수 있습니다.
        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN
        texts.append(text)
    return {
        "text": texts,  # 포맷팅된 텍스트를 반환합니다.
    }


# 데이터셋에 formatting_prompts_func 함수를 적용합니다. 배치 처리를 활성화합니다.
train_ds = train_ds.map(
    formatting_prompts_func,
    batched=True,
)

eval_ds = eval_ds.map(
    formatting_prompts_func,
    batched=True,
)
```

```
print(train_ds[0], '\n', eval_ds[0])
```

### 7. Huggingface SFTTrainer를 사용한 모델 훈련하기
```
from trl import SFTTrainer
from transformers import TrainingArguments, TrainerCallback, TrainerState, TrainerControl
from transformers import EarlyStoppingCallback
import torch

class CustomEarlyStoppingCallback(EarlyStoppingCallback):
    def on_train_end(self, args, state, control, **kwargs):
        super().on_train_end(args, state, control, **kwargs)
        if control.should_training_stop:
            print(f"\nEarly stopping criteria met at step {state.global_step}. Training stopped.")

tokenizer.padding_side = "right"  # 토크나이저의 패딩을 오른쪽으로 설정합니다.

# SFTTrainer를 사용하여 모델 학습 설정
trainer = SFTTrainer(
    model=model,  # 학습할 모델
    tokenizer=tokenizer,  # 토크나이저
    train_dataset=train_ds,  # 학습 데이터셋
    eval_dataset=eval_ds,# eval dataset
    dataset_text_field="text",  # 데이터셋에서 텍스트 필드의 이름
    max_seq_length=max_seq_length,  # 최대 시퀀스 길이
    dataset_num_proc=2,  # 데이터 처리에 사용할 프로세스 수
    packing=False,  # 짧은 시퀀스에 대한 학습 속도를 5배 빠르게 할 수 있음
    args=TrainingArguments(
        per_device_train_batch_size=2,  # 각 디바이스당 훈련 배치 크기
        gradient_accumulation_steps=4,  # 그래디언트 누적 단계
        warmup_steps=5,  # 웜업 스텝 수
        # you can set num_train_epochs=1 for a full run, and turn off max_steps=None
        # 공식문서에는 없음.
        num_train_epochs=3,  # 훈련 에폭 수
        max_steps=100,  # 최대 스텝 수
        do_eval=True,
        eval_strategy="steps", # from https://github.com/huggingface/setfit/issues/512
        logging_steps=5,  # logging 스텝 수
        learning_rate=2e-4,  # 학습률
        fp16=not torch.cuda.is_bf16_supported(),  # fp16 사용 여부, bf16이 지원되지 않는 경우에만 사용
        bf16=torch.cuda.is_bf16_supported(),  # bf16 사용 여부, bf16이 지원되는 경우에만 사용
        optim="adamw_8bit",  # 최적화 알고리즘
        weight_decay=0.01,  # 가중치 감소
        lr_scheduler_type="cosine",  # 학습률 스케줄러 유형 # 공식은 linear
        seed=123,  # 랜덤 시드 # 공식은 3407
        output_dir="outputs",  # 출력 디렉토리
        report_to="none",
        load_best_model_at_end=True, # Early Stopping을 위해 추가
        greater_is_better=False, # Early Stopping을 위해 추가 (loss는 작을수록 좋으므로 False)
    ),
    callbacks=[CustomEarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)] # validation loss가 3번 연속 0.001 이상 증가 하면 Custom Early Stopping Callback 사용
```

#### 차트
```
%%time
trainer_stats = trainer.train()  # 모델을 훈련시키고 통계를 반환합니다.
```

#### 그래프
```
import matplotlib.pyplot as plt
import pandas as pd

# 훈련 과정 중 기록된 로그 히스토리 가져오기
log_history = trainer.state.log_history

# 훈련 손실과 검증 손실 데이터 추출
train_loss = []
eval_loss = []
train_steps = []
eval_steps = []

for log in log_history:
    if "loss" in log:
        train_loss.append(log["loss"])
        train_steps.append(log["step"])
    if "eval_loss" in log:
        eval_loss.append(log["eval_loss"])
        eval_steps.append(log["step"]) # Use the step associated with the evaluation

# 그래프 그리기
plt.figure(figsize=(10, 6))

if train_loss:
    plt.plot(train_steps, train_loss, label="Training Loss")

if eval_loss:
    plt.plot(eval_steps, eval_loss, label="Validation Loss")

plt.xlabel("Steps")
plt.ylabel("Loss")
plt.title("Training and Validation Loss over Steps")
plt.legend()
plt.grid(True)
plt.show()
```

### 8. 모델의 예측 성능 검증
```
from transformers import StoppingCriteria, StoppingCriteriaList


class StopOnToken(StoppingCriteria):
    def __init__(self, stop_token_id):
        self.stop_token_id = stop_token_id  # 정지 토큰 ID를 초기화합니다.

    def __call__(self, input_ids, scores, **kwargs):
        return (
            self.stop_token_id in input_ids[0]
        )  # 입력된 ID 중 정지 토큰 ID가 있으면 정지합니다.


# end_token을 설정
stop_token = "<|end_of_text|>"  # end_token으로 사용할 토큰을 설정합니다.
stop_token_id = tokenizer.encode(stop_token, add_special_tokens=False)[
    0
]  # end_token의 ID를 인코딩합니다.

# Stopping criteria 설정
stopping_criteria = StoppingCriteriaList(
    [StopOnToken(stop_token_id)]
)  # 정지 조건을 설정합니다.
```

### 파인튜닝한 LLM과 Q&A 예시 1
```
# alpaca_prompt = Copied from above
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
inputs = tokenizer(
[
    alpaca_prompt.format(
        "아이스크림이 있는 곳은?", # instruction
        #"테디노트 유튜브 채널에 대해 알려주세요.", # instruction
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 4096, use_cache = True)
tokenizer.batch_decode(outputs)
```

### 파인튜닝한 LLM과 Q&A 예시 2
```
from transformers import TextStreamer

inputs = tokenizer(
    [
        alpaca_prompt.format(
            "현금으로 결제가 가능한가요?",  # 지시사항
            "",  # 출력 - 생성을 위해 이 부분을 비워둡니다!
        )
    ],
    return_tensors="pt",
).to("cuda")


text_streamer = TextStreamer(tokenizer)
_ = model.generate(
    **inputs,
    streamer=text_streamer,
    max_new_tokens=4096,  # 최대 생성 토큰 수를 설정합니다.
)
```

### 파인튜닝한 LLM과 Q&A 예시 3
```
from transformers import TextStreamer

inputs = tokenizer(
    [
        alpaca_prompt.format(
            "이 곳에서 판매하는 상품은 어떤 것들이 있나요?",  # 지시사항
            "",  # 출력 - 생성을 위해 이 부분을 비워둡니다!
        )
    ],
    return_tensors="pt",
).to("cuda")


text_streamer = TextStreamer(tokenizer)
_ = model.generate(
    **inputs,
    streamer=text_streamer,
    max_new_tokens=4096,  # 최대 생성 토큰 수를 설정합니다.
    stopping_criteria=stopping_criteria  # 생성을 멈출 기준을 설정합니다.
)
```

### 9. LoRA adapter 저장
```
# LoRA 어댑터 모델이 저장될 로컬 경로명 입력
model.save_pretrained("loda-adapter")  # 모델을 로컬에 저장합니다.
```

### 10. 파인튜닝한 모델을 허깅페이스에 저장하기
```
save_method='merged_16bit'
```

```
huggingface_repo="나의 huggingface repo 이름"
hf_token="huggingface write 토큰 값"
```

```
%time
model.push_to_hub_merged(
    huggingface_repo,
    tokenizer,
    save_method=save_method,
    token=hf_token,
)
```

### 확인 

<img width="1064" height="649" alt="image" src="https://github.com/user-attachments/assets/5b1bf2f4-a4a2-4bb3-8a06-fdebc844bf78" />



