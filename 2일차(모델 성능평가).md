# 파인튜닝한 모델 성능평가
### 1. unsloth 설치 
```
%%capture
import os, re
if "COLAB_" not in "".join(os.environ.keys()):
    !pip install unsloth
else:
    # Do this only in Colab notebooks! Otherwise use pip install unsloth
    import torch; v = re.match(r"[0-9\.]{3,}", str(torch.__version__)).group(0)
    xformers = "xformers==" + ("0.0.32.post2" if v == "2.8.0" else "0.0.29.post3")
    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo
    !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" "huggingface_hub>=0.34.0" hf_transfer
    !pip install --no-deps unsloth
!pip install transformers==4.55.4
!pip install --no-deps trl==0.22.2
```

### 2. 파인튜닝한 모델을 기동하기
- 허깅페이스 모델 repository에 저장하였던 모델을 다시 불러오기

```
from unsloth import FastLanguageModel
import torch

# 모델 이름 설정
model_name = "huggingface 모델 이름"
max_seq_length=4096

# Unsloth를 사용하여 모델 로드
# 필요에 따라 max_seq_length, dtype, load_in_4bit 등의 인자를 조절하세요.
unsloth_model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length, # 예시 값, 필요에 따라 조정
    dtype=None,          # 자동 감지 또는 필요에 따라 Float16, Bfloat16 설정
    load_in_4bit=True,   # 메모리 절약을 위해 4bit 양자화 로드 여부 설정
)

print(f"'{model_name}' 모델이 'unsloth_model' 변수로 로드되었습니다.")
```

```
# EOS_TOKEN은 문장의 끝을 나타내는 토큰입니다. 이 토큰을 추가해야 합니다.
EOS_TOKEN = tokenizer.eos_token

# AlpacaPrompt를 사용하여 지시사항을 포맷팅하는 함수입니다.
alpaca_prompt = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{}

### Response:
{}"""
```

#### copy 확인 
```
# alpaca_prompt = Copied from above
FastLanguageModel.for_inference(unsloth_model) # Enable native 2x faster inference
inputs = tokenizer(
[
    alpaca_prompt.format(
        "아이스크림이 있는 곳은?", # instruction
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

outputs = unsloth_model.generate(**inputs, max_new_tokens = 4096, use_cache = True)
tokenizer.batch_decode(outputs)
```

### 3. 평가용 데이터 셋 로드 
- 데이터 셋을 9:1로 split 했을 때 1에 해당하는 부분 로드 

```
from datasets import load_dataset
eval_ds = load_dataset("hugging 모델 이름", data_files="test_ds.csv", split="train")
# Convert the datasets object to a pandas DataFrame
eval_df = eval_ds.to_pandas()
# Display the head of the resulting DataFrame
eval_df.head()
```

<img width="828" height="217" alt="image" src="https://github.com/user-attachments/assets/5e3cdf0e-c855-4970-a669-6a9d35fab862" />


### 4. 추론을 위한 데이터 준비
```
prompts = eval_df['instruction'].tolist()
ground_truth_responses = eval_df['output'].tolist()
```

```
prompts[:5]  
```


<img width="629" height="137" alt="image" src="https://github.com/user-attachments/assets/5402a304-f7a3-409f-9b96-a763130fcd9d" />


```
ground_truth_responses[:5]
```

<img width="644" height="130" alt="image" src="https://github.com/user-attachments/assets/b276e0d6-4af5-4464-b0d3-e9e4acc723a3" />


### 5. prompts를 모델에 입력하여 응답을 생성합니다. 생성된 응답의 처음 5개를 출력하여 확인
```
max_seq_length=4096
```

```
# Regenerate responses
generated_responses = []
for prompt in prompts:
    inputs = tokenizer(
        [
            alpaca_prompt.format(
                prompt, # instruction
                "", # output - leave this blank for generation!
            )
        ], return_tensors = "pt").to("cuda")

    outputs = unsloth_model.generate(
        **inputs,
        max_new_tokens=4096,
        use_cache=True,
    )
    # Decode the generated tokens
    decoded_output = tokenizer.batch_decode(outputs)

    #print('Printing decode_output: ', decoded_output)

    # Extract only the response part by splitting at "### Response:\n"
    response = decoded_output[0].split("### Response:\n")[-1].strip()
    # Remove the EOS token if it exists at the end
    if response.endswith(tokenizer.eos_token):
        response = response[:-len(tokenizer.eos_token)].strip()

    generated_responses.append(response)


print("First 5 generated responses")
for i, response in enumerate(generated_responses[:5]):
    print(f"Response {i+1}: {response}")
```


### 6. bertscore 계산을 위한 준비
```
!pip -q install bert-score       //pip instal 명령어를 사용하여 bert-score 라이브러리를 설치
```

### 7. bertscore 계산
```
from bert_score import score        // bert_score 라이브러리의 score 함수를 사용하여 생성된 응답과 실제 응답 간의 BERTScore를 계산

# Calculate BERTScore
precision, recall, f1 = score(generated_responses, ground_truth_responses, lang="ko")

# Print the average BERTScore
print(f"BERT Score Precision: {precision.mean().item():.3f}, \n")
print(f"BERT Score Recall: {recall.mean().item():.3f},\n")
print(f"BERT Score F1: {f1.mean().item():.3f}")
```

### 8.  계산한 BERT Score F1을 csv 파일로 저장
```
import pandas as pd

# Create a DataFrame to store the scores if it doesn't exist
if 'score' not in locals():
    score = pd.DataFrame(columns=['BERT Score F1'])

# Append the BERT Score F1 value to the DataFrame
f1_value = f1.mean().item()

score = pd.concat([score, pd.DataFrame([{'BERT Score F1': f1_value}])], ignore_index=True)

# Display the updated DataFrame
display(score)

# save to csv
```


### 9. 결과 

<img width="542" height="597" alt="image" src="https://github.com/user-attachments/assets/eb28be3f-371b-4847-a8ee-e765861d0fae" />



